# LabRepoStarterKit

One of the main challenges posed by social robots is how to guarantee their adherence to human social norms and values, including the standards and expectations of humans who will be in close relationship with those robots. In this respect, it is essential that social robots can understand human moral considerations and make decisions about what is appropriate in a given context. In this study, we aim to evaluate how implementing different moral decision-making frameworks in social robots changes people's perception of artificial systems as intentional agents, and as a consequence, increases people‚Äôs trust and likelihood of choosing social robots as cooperative partners. For that, we recruited 250 online panelists, who conveyed their level of agreement with decisions made by social robots using either a utilitarian, deontic, or contractualist moral framework, as well as a non-moral framework. Next, we asked participants to rate perceived reliability, moral trust, and moral agency of each of the social robots. As a comparison, we also asked participants to rate their agreement with moral decisions made by humans using the same three moral frameworks and a non-moral framework. Our results have the potential to impact the future of social robot design, as we generateed insights into how different ethical frameworks foster positive relationships between humans and robots based on how they navigate moral decisions.


### üìÅ What's Included

- **Validation script**: A Markdown file containing the code used to validate the moral dilemmas presented in the study.
- **Main analysis script**: A Markdown file with the code used to run and analyze the behavioral study. 


